{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e0253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ Run started at 2025-06-19 22:54:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\559680733.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\559680733.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\559680733.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\559680733.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table truncated and refreshed.\n",
      "‚úÖ DBT models refreshed.\n",
      "üì© Email sent to DMCABJ22001599 (emmanueljjustice@gmail.com)\n",
      "üì© Email sent to DMCABJ22001600 (emmanueljjustice@gmail.com)\n",
      "üì© Email sent to DMCABJ22001601 (emmanueljjustice@gmail.com)\n",
      "üì© Email sent to DMCABJ22001603 (emmanueljjustice@gmail.com)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "import os\n",
    "import pickle\n",
    "import base64\n",
    "import re\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from email.mime.text import MIMEText\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------\n",
    "# CONFIG FROM ENVIRONMENT\n",
    "# ---------------------\n",
    "\n",
    "sheet_sources = {\n",
    "    \"Lagos Midweek\": os.getenv(\"SHEET_LAGOS_MIDWEEK\"),\n",
    "    \"Lagos Weekend\": os.getenv(\"SHEET_LAGOS_WEEKEND\"),\n",
    "    \"Abuja Midweek\": os.getenv(\"SHEET_ABUJA_MIDWEEK\"),\n",
    "    \"Abuja Weekend\": os.getenv(\"SHEET_ABUJA_WEEKEND\")\n",
    "}\n",
    "\n",
    "sheet_metadata = {\n",
    "    \"Lagos Midweek\": {\"location\": \"Lagos\", \"course\": \"Data Analytics\"},\n",
    "    \"Lagos Weekend\": {\"location\": \"Lagos\", \"course\": \"Data Analytics\"},\n",
    "    \"Abuja Midweek\": {\"location\": \"Abuja\", \"course\": \"Data Analytics\"},\n",
    "    \"Abuja Weekend\": {\"location\": \"Abuja\", \"course\": \"Data Analytics\"}\n",
    "}\n",
    "\n",
    "# PostgreSQL DB connection from environment\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = quote_plus(os.getenv('DB_PASSWORD'))\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_port = os.getenv('DB_PORT', '5432')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "connection_str = f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "engine = create_engine(connection_str)\n",
    "\n",
    "# Gmail API config\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.send']\n",
    "CLIENT_SECRET_FILE = os.getenv('CLIENT_SECRET_FILE')\n",
    "TOKEN_FILE = 'token.pickle'\n",
    "\n",
    "# DBT project path\n",
    "DBT_PROJECT_PATH = os.getenv('DBT_PROJECT_PATH')\n",
    "\n",
    "# ---------------------\n",
    "# DATA CLEANING FUNCTIONS\n",
    "# ---------------------\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "        .str.replace(r'\\s+', '_', regex=True)\n",
    "        .str.replace(r'__+', '_', regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def clean_dates(df, date_cols):\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
    "    return df\n",
    "\n",
    "def better_clean_numerics(df, num_cols):\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)\n",
    "            df[col] = df[col].replace('', None)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# ---------------------\n",
    "# FULL DATA PIPELINE\n",
    "# ---------------------\n",
    "\n",
    "def refresh_data():\n",
    "    logger.info(\"Starting data refresh...\")\n",
    "    dfs = []\n",
    "    for source, url in sheet_sources.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                df = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "                df = clean_column_names(df)\n",
    "                df[\"source\"] = source\n",
    "                df[\"location\"] = sheet_metadata[source][\"location\"]\n",
    "                df[\"course\"] = sheet_metadata[source][\"course\"]\n",
    "                dfs.append(df)\n",
    "                logger.info(f\"‚úÖ Successfully loaded: {source}\")\n",
    "            else:\n",
    "                logger.error(f\"‚ùå Failed to load: {source} (Status: {response.status_code})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading {source}: {e}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        logger.error(\"No data loaded from any source!\")\n",
    "        return None\n",
    "    \n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    date_columns = ['timestamp', 'payment_date', 'payment_date_2nd', 'payment_date_3rd', 'start_date', 'ssdate', 'sub_end']\n",
    "    numeric_columns = ['amount', '1st_installment', '2nd', '3rd', 'sub_days', 'sub_left', 'on_a_scale_of_1_10_what_is_your_skill_level_in_data_analytics']\n",
    "\n",
    "    full_df = clean_dates(full_df, date_columns)\n",
    "    full_df = better_clean_numerics(full_df, numeric_columns)\n",
    "    full_df = full_df.where(pd.notnull(full_df), None)\n",
    "\n",
    "    logger.info(f\"Data refresh complete. Total rows: {len(full_df)}\")\n",
    "    return full_df\n",
    "\n",
    "def load_to_postgres(df):\n",
    "    if df is None or df.empty:\n",
    "        logger.error(\"Cannot load empty dataframe to PostgreSQL\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(\"TRUNCATE TABLE global_table;\"))\n",
    "        df.to_sql('global_table', engine, if_exists='append', index=False, method='multi')\n",
    "        logger.info(\"‚úÖ Table truncated and refreshed.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load to PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_dbt():\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"dbt\", \"run\", \"--full-refresh\"], \n",
    "            cwd=DBT_PROJECT_PATH,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        logger.info(\"‚úÖ DBT models refreshed.\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"‚ùå DBT run failed: {e.stderr}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error running DBT: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_milestone_candidates():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=db_name, \n",
    "            user=db_user, \n",
    "            password=os.getenv('DB_PASSWORD'), \n",
    "            host=db_host, \n",
    "            port=db_port\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            candid, \n",
    "            start_date, \n",
    "            adjusted_end_date, \n",
    "            (\n",
    "                (sub_days + COALESCE(paused_days, 0) + COALESCE(extra_days, 0)) \n",
    "                - (CURRENT_DATE - start_date)\n",
    "            ) AS days_left, \n",
    "            email\n",
    "        FROM subscription_status\n",
    "        WHERE (\n",
    "                (sub_days + COALESCE(paused_days, 0) + COALESCE(extra_days, 0)) \n",
    "                - (CURRENT_DATE - start_date)\n",
    "              ) IN (90, 60, 30, 10, 0)\n",
    "           OR start_date = CURRENT_DATE\n",
    "        \"\"\"\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        logger.info(f\"Found {len(data)} milestone candidates\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error fetching milestone candidates: {e}\")\n",
    "        return []\n",
    "\n",
    "# ---------------------\n",
    "# GMAIL API AUTH\n",
    "# ---------------------\n",
    "\n",
    "def gmail_auth():\n",
    "    try:\n",
    "        creds = None\n",
    "        if os.path.exists(TOKEN_FILE):\n",
    "            with open(TOKEN_FILE, 'rb') as token:\n",
    "                creds = pickle.load(token)\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "            with open(TOKEN_FILE, 'wb') as token:\n",
    "                pickle.dump(creds, token)\n",
    "        logger.info(\"‚úÖ Gmail authentication successful\")\n",
    "        return build('gmail', 'v1', credentials=creds)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Gmail authentication failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------------\n",
    "# SEND EMAIL\n",
    "# ---------------------\n",
    "\n",
    "def send_email_gmail(name, days_left, email, service):\n",
    "    if service is None:\n",
    "        logger.error(\"Gmail service not initialized\")\n",
    "        return False\n",
    "    \n",
    "    if days_left == 90:\n",
    "        body = f\"\"\"Hi {name},\\n\\nWelcome to Datapluga! Your 90-day subscription begins today!\\n\"\"\"\n",
    "    elif days_left == 60:\n",
    "        body = f\"Hi {name}, 30 days in! Keep pushing üí™.\"\n",
    "    elif days_left == 30:\n",
    "        body = f\"Hi {name}, 60 days down, just 30 more to go.\"\n",
    "    elif days_left == 10:\n",
    "        body = f\"Hi {name}, only 10 days left. Wrap up strong!\"\n",
    "    elif days_left == 0:\n",
    "        body = f\"Hi {name}, your subscription has expired. Please consider renewing.\"\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    message = MIMEText(body)\n",
    "    message['to'] = email\n",
    "    message['from'] = 'me'\n",
    "    message['subject'] = 'üì¨ Datapluga Subscription Update'\n",
    "\n",
    "    raw_message = {'raw': base64.urlsafe_b64encode(message.as_bytes()).decode()}\n",
    "\n",
    "    try:\n",
    "        service.users().messages().send(userId='me', body=raw_message).execute()\n",
    "        logger.info(f\"üì© Email sent to {name} ({email})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to send email to {name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# ---------------------\n",
    "# MAIN WORKFLOW\n",
    "# ---------------------\n",
    "\n",
    "def main():\n",
    "    logger.info(f\"\\n‚è∞ Pipeline started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Refresh data\n",
    "        df = refresh_data()\n",
    "        if df is None:\n",
    "            logger.error(\"Data refresh failed. Aborting pipeline.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 2: Load to PostgreSQL\n",
    "        if not load_to_postgres(df):\n",
    "            logger.error(\"PostgreSQL load failed. Aborting pipeline.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 3: Run DBT\n",
    "        if not run_dbt():\n",
    "            logger.error(\"DBT run failed. Aborting pipeline.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 4: Send emails\n",
    "        service = gmail_auth()\n",
    "        if service:\n",
    "            milestones = get_milestone_candidates()\n",
    "            for row in milestones:\n",
    "                candid, start_date, end_date, days_left, email = row\n",
    "                send_email_gmail(candid, days_left, email, service)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Pipeline completed successfully at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Pipeline failed with error: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8016f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
